{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227578"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r', encoding='utf-8').read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3139299"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # TODO\n",
    "    # make lowercase\n",
    "    # replace all punctuation except dots with spaces\n",
    "    # collapse multiple spaces into one '   ' -> ' '\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z_\\.]', r' ', text)\n",
    "    text = re.sub(r'\\s+', r' ', text)\n",
    "    return text\n",
    "\n",
    "#%%time\n",
    "\n",
    "text = preprocess_text(text)\n",
    "len(text)\n",
    "#assert len(text) == 3141169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split('.')\n",
    "text = [x.strip() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        text: list of strings \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
    "        \n",
    "        text_line = ''\n",
    "        for t in text:\n",
    "            text_line += t\n",
    "        list_text = list(text_line)\n",
    "        self.itos = [a for a in set(list_text)]\n",
    "        indices = [i for i in range(len(self.itos))]\n",
    "        self.stoi = {k:v for k, v in zip(self.itos, indices)}\n",
    "        bigram_freqs = Counter()\n",
    "        for t in text:\n",
    "            bigram_freqs.update(zip(t, t[1:]))\n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            # TODO\n",
    "            # count bigram freqencies in the text\n",
    "            try:\n",
    "                new_token = bigram_freqs.most_common(1)[0][0]# most common bigram in the text\n",
    "            except:\n",
    "                break\n",
    "            new_id = len(self.itos)\n",
    "            new_token = ''.join([str(i) for i in new_token])\n",
    "            self.itos.append(new_token)\n",
    "            self.stoi[new_token] = new_id\n",
    "            # find occurences of the new_token in the text and replace them with new_id\n",
    "            new_text = []\n",
    "            for t in text:\n",
    "                new_t = []\n",
    "                i = 0\n",
    "                while i < len(t)-1:\n",
    "                #for i in range(len(t)-1):\n",
    "                    el, next_el = t[i], t[i+1]\n",
    "                    bigram = [str(el), str(next_el)]\n",
    "                    bigram = ''.join(bigram)\n",
    "                    if bigram == new_token:\n",
    "                        new_t.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_t.append(el)\n",
    "                        i += 1\n",
    "                new_text.append(new_t)\n",
    "            text = new_text\n",
    "            bigram_freqs = Counter()\n",
    "            for t in text:\n",
    "                bigram_freqs.update(zip(t, t[1:]))\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        convert text to a sequence of token ids\n",
    "        text: list of strings\n",
    "        \"\"\"\n",
    "        keys = list(self.stoi.keys())\n",
    "        keys_len_sorted = sorted(keys, key=len, reverse=True)\n",
    "        new_texts = []\n",
    "        for t in text:\n",
    "            new_t = []\n",
    "            while len(t) > 0:\n",
    "                for k in keys_len_sorted:\n",
    "                    if t.startswith(k):\n",
    "                        len_k = len(k)\n",
    "                        new_t.append(self.stoi[k])\n",
    "                        t = t[len_k:]\n",
    "\n",
    "            new_texts.append(new_t)\n",
    "        text = new_texts    \n",
    "            \n",
    "                    \n",
    "        #for token_id, token in enumerate(self.itos):\n",
    "            # find occurences of the token in the text and replace them with token_id\n",
    "            #text = # TODO\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def decode_token(self, tok): \n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        result = self.itos[tok]# TODO\n",
    "        return result\n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "var = bpe.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = bpe.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LM at 0x1ba84b16240>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "        \n",
    "    \n",
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "\n",
    "  \n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        self.proba = Counter()# TODO create array for storing 3-gram counters\n",
    "        \n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        freq_list = []\n",
    "        freq_sum = 0\n",
    "        for w in range(self.vocab_size):\n",
    "            w_counter = self.get_proba(a, b, w, tau=tau)\n",
    "            freq_list.append(w_counter)\n",
    "            freq_sum += w_counter\n",
    "        result = [i/freq_sum for i in freq_list]\n",
    "        return np.array(result)\n",
    "        \n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        trigram_proba =  self.proba[(a, b, c)]\n",
    "        result = (trigram_proba + self.delta) ** (1/tau) # TODO approximate probability by counters\n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        for t in text:\n",
    "            t = [start_token] + [t_el for t_el in t] + [end_token]\n",
    "            self.proba.update((zip(t, t[1:], t[2:])))# TODO count 3-grams in the text\n",
    "        return self\n",
    "\n",
    "lm = LM(vocab_size, 1)\n",
    "lm.fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "    beam = [(input_seq, 0)] # TODO store in beam tuples of current sequences and their log probabilities\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        for snt, snt_proba in beam:\n",
    "            if np.argmax(snt[-1]) == end_token:\n",
    "                continue\n",
    "            else:\n",
    "                proba = lm.infer(snt[-2], snt[-1], tau) # probability vector of the next token\n",
    "                best_k = proba.argsort()[-k:][::-1] # top-k most probable tokens\n",
    "                # TODO update candidates' sequences and corresponding probabilities\n",
    "                candidates += [snt + [int(x)] for x in best_k]\n",
    "                candidates_proba += [snt_proba + x for x in np.log(proba[best_k])]\n",
    "\n",
    "        beam = [(candidates[i], candidates_proba[i]) for i in np.argsort(candidates_proba).astype(int)[-k:][::-1]]\n",
    "                # select top-k most probable sequences from candidates\n",
    "    return beam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: horse of the was and , probability: 0.2625549047590843\n",
      "sequence: horse of the said no, probability: 0.14853518166872237\n",
      "sequence: horse the the was , probability: 0.14273488000514523\n",
      "sequence: horse of the saing, probability: 0.09892398490650087\n",
      "sequence: horse the the said , probability: 0.0720614794748892\n",
      "sequence: horse and the was , probability: 0.05750507492785594\n",
      "sequence: horse the the sain, probability: 0.04748967620059593\n",
      "sequence: horse and the said , probability: 0.02903215231249907\n",
      "sequence: horse and the sain, probability: 0.019132656209305265\n",
      "sequence: horse of the was of , probability: 0.016910725826546534\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(f'sequence: {bpe.decode(snt)}, probability: {np.exp(snt_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: here the was and , probability: 0.41367929316290586\n",
      "sequence: here the said no, probability: 0.2340307792723301\n",
      "sequence: here the saing, probability: 0.1558637961478164\n",
      "sequence: here she was and , probability: 0.027144450436631204\n",
      "sequence: here the was of , probability: 0.026644396962290703\n",
      "sequence: here the count , probability: 0.025227229051286458\n",
      "sequence: here the was h, probability: 0.019381973344904452\n",
      "sequence: here the had th, probability: 0.015484186676600209\n",
      "sequence: here she said no, probability: 0.015356429469875086\n",
      "sequence: here she saing, probability: 0.010227335908093178\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(f'sequence: {bpe.decode(snt)}, probability: {np.exp(snt_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: whation of the was, probability: 7.779316442905634e-06\n",
      "sequence: whattle of the was, probability: 6.339932447711428e-06\n",
      "sequence: what of the the had, probability: 5.526646780577358e-06\n",
      "sequence: what of the the wa, probability: 4.777317364557427e-06\n",
      "sequence: what ing ing h, probability: 4.463774536636801e-06\n",
      "sequence: whation of the sai, probability: 3.7627873219074915e-06\n",
      "sequence: what of the the sa, probability: 3.190163551834125e-06\n",
      "sequence: what ing ing i, probability: 2.307784299344787e-06\n",
      "sequence: what of the the wi, probability: 2.1292568344294423e-06\n",
      "sequence: what ing ing and , probability: 1.6954624897081563e-06\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(f'sequence: {bpe.decode(snt)}, probability: {np.exp(snt_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: gun he was and she , probability: 0.19590948446569714\n",
      "sequence: gun he said not h, probability: 0.09587814834913423\n",
      "sequence: gun his she was , probability: 0.0659205058685809\n",
      "sequence: gun had be she wa, probability: 0.054913760604837054\n",
      "sequence: gun had be she sa, probability: 0.0459949375005684\n",
      "sequence: gun he saing he , probability: 0.04380554609149823\n",
      "sequence: gun he said not i, probability: 0.03362248340485744\n",
      "sequence: gun his she said , probability: 0.03328078729215884\n",
      "sequence: gun he saing his, probability: 0.025163471271724882\n",
      "sequence: gun his he was and , probability: 0.022268451271812456\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "# TODO print decoded generated strings and their probabilities\n",
    "for (snt, snt_proba) in result:\n",
    "    print(f'sequence: {bpe.decode(snt)}, probability: {np.exp(snt_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18389452209314572"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "def perplexity(snt, lm):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "\n",
    "    sigma = 0\n",
    "    \n",
    "    snt = [start_token] + snt + [end_token]\n",
    "\n",
    "    for char in range(len(snt) - 2):\n",
    "        sigma += log((1 / lm.infer(snt[char], snt[char + 1])[snt[char + 2]]))\n",
    "    factor = (-1 / float(len(snt))) * sigma\n",
    "    result = 2 ** factor\n",
    "    return result \n",
    "\n",
    "perplexity(tokenized_text[0], lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
